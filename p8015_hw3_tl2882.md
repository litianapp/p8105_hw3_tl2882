p8105\_hw3\_tl2882
================
Tian Li
2018-10-09

``` r
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")
```

    ## Skipping install of 'p8105.datasets' from a github remote, the SHA1 (21f5ad1c) has not changed since last install.
    ##   Use `force = TRUE` to force installation

``` r
library(p8105.datasets)
```

Problem 1
=========

Read and clean brfss\_smart2010
-------------------------------

``` r
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) %>% 
  arrange(year, locationdesc, response)
```

Problem 1.1
-----------

``` r
brfss %>%  
  filter(year == "2002") %>% 
  group_by(locationabbr) %>%
  summarize(n_locations = n_distinct(locationdesc)) %>% 
  filter(n_locations == 7)
```

    ## # A tibble: 3 x 2
    ##   locationabbr n_locations
    ##   <chr>              <int>
    ## 1 CT                     7
    ## 2 FL                     7
    ## 3 NC                     7

In 2002, CT, FL, and NC states were observed at 7 locations.

Problem 1.2
-----------

``` r
brfss %>%  
  group_by(year, locationabbr) %>%
  summarize(n_locations = n_distinct(locationdesc)) %>%
  ggplot(aes(x = year, y = n_locations, color = locationabbr)) + 
    geom_line() + 
    labs(title = "The number of locations in each state from 2002 to 2010",
         x = "Year", 
         y = "Number of locations", 
         caption = "Data from the p8105.datasets") + 
    viridis::scale_color_viridis(name = "State", discrete = TRUE) +
    theme(legend.position = "right")
```

<img src="p8015_hw3_tl2882_files/figure-markdown_github/Problem_1_2-1.png" width="90%" />

This is the “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.
Y is "Number of observations", X is "Year", and each line represents a different state.

Problem 1.3
-----------

``` r
brfss %>%  
  filter(year == "2002" | year == "2006" | year == "2010") %>% 
  filter(locationabbr == "NY", response == "Excellent") %>%
  group_by(year) %>% 
  summarize(excellent_mean = mean(data_value),
            excellent_sd = sd(data_value)) %>% 
  knitr::kable(digits = 1)
```

|  year|  excellent\_mean|  excellent\_sd|
|-----:|----------------:|--------------:|
|  2002|             24.0|            4.5|
|  2006|             22.5|            4.0|
|  2010|             22.7|            3.6|

This is the table showing, for the years 2002, 2006, and 2010, the mean (as in "excellent\_mean") and standard deviation (as in "excellent\_sd") of the proportion of “Excellent” responses across locations in NY State.

Problem 1.4
-----------

``` r
brfss %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(proportion_mean = mean(data_value)) %>% 
  ggplot(aes(x = year, y = proportion_mean, color = locationabbr)) + 
    geom_line() + 
    labs(
      title =
        "Distribution of these state-level averages over time (for each response separately)",
      caption = "Data from the p8105.datasets") + 
    theme(axis.text.x = element_text(angle = 90)) +
    facet_grid(~response) + 
    viridis::scale_fill_viridis(discrete = TRUE)
```

    ## Warning: Removed 1 rows containing missing values (geom_path).

<img src="p8015_hw3_tl2882_files/figure-markdown_github/Problem_1_4-1.png" width="90%" />

-   I chose "geom\_line" because line chart shows the trend of data at equal time intervals, which reflects the distribution of these state-level averages over time best.

This is the five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

Problem 2
=========

``` r
head(instacart)
```

    ## # A tibble: 6 x 15
    ##   order_id product_id add_to_cart_ord… reordered user_id eval_set
    ##      <int>      <int>            <int>     <int>   <int> <chr>   
    ## 1        1      49302                1         1  112108 train   
    ## 2        1      11109                2         1  112108 train   
    ## 3        1      10246                3         0  112108 train   
    ## 4        1      49683                4         0  112108 train   
    ## 5        1      43633                5         1  112108 train   
    ## 6        1      13176                6         0  112108 train   
    ## # ... with 9 more variables: order_number <int>, order_dow <int>,
    ## #   order_hour_of_day <int>, days_since_prior_order <int>,
    ## #   product_name <chr>, aisle_id <int>, department_id <int>, aisle <chr>,
    ## #   department <chr>

Exploration of this dataset
---------------------------

This is data from a deliver company named Instacart.

It is a 1,384,617 rows X 15 columns data frame, which means it has 1,384,617 observations and 15 variables.

The tpypes of "eval\_set", "product\_name", "aisle" and "department" are "character", others are "int".

Key variable: "order\_id" and "product\_id".
With the order ID and product ID (or product name), we can specify every observation.

Illustrative examples of observations:
For example,
for order ID 1 (order number 4), it means that a person (user ID 112108) orderd 8 items at 10:00, Thursday, 9 days after last order.

Those items are: "Bulgarian Yogurt", "Organic 4% Milk Fat Whole Milk Cottage Cheese", "Organic Celery Hearts", "Cucumber Kirby", "Lightly Smoked Sardines in Olive Oil", "Bag of Organic Bananas", "Organic Hass Avocado", "Organic Whole String Cheese".

They are from aisle "yogurt", "other creams cheeses", "fresh vegetables", "fresh vegetables", "canned meat seafood", "fresh fruits", "fresh fruits", "packaged cheese" respectively.

They are from department "dairy eggs", "dairy eggs", "produce", "produce", "canned goods", "produce", "produce", "dairy eggs" respectively.

Problem 2.1
-----------

``` r
unique(instacart$aisle) %>% 
  length()
```

    ## [1] 134

``` r
instacart %>% 
  count(aisle) %>% 
  filter(min_rank(desc(n)) == 1)
```

    ## # A tibble: 1 x 2
    ##   aisle                 n
    ##   <chr>             <int>
    ## 1 fresh vegetables 150609

There are 134 aisles, and the "fresh vegetables" is the most items ordered from.
